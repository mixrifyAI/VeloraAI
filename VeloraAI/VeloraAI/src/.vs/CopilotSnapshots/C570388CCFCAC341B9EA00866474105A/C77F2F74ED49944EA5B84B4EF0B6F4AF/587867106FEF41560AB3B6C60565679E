// 𝗩𝗲𝗹𝗼𝗿𝗮𝗔𝗜
// uses LLamaSharp for AI model interaction.
// is a C# library for interacting with AI models that are pre-destined and downloaded for quick optimal use through our own Asynchronous method.
// supports using your own AI models through TestingMode. Turn on TestingMode, and define TestingModelPath to use your own model.
// is designed to be used in .NET 8.0 applications.

using System;
using System.Collections.Generic;
using System.Diagnostics;
using System.IO;
using System.Net.Http;
using System.Threading.Tasks;
using LLama;
using LLama.Common;
using LLama.Sampling;

namespace VeloraAI
{
    public static class VeloraAI
    {
        static LLamaContext _context;
        static InteractiveExecutor _executor;
        static ChatSession _session;
        private static string _modelPath = "";
        private static Models? _currentModel = null;

        public static event EventHandler<string> TextGenerated = delegate { };
        public static event EventHandler ResponseStarted = delegate { };
        public static event EventHandler ResponseEnded = delegate { };

        private static bool Authenticated = false;

        public static bool TestingMode { get; set; } = false;
        public static string TestingModelPath { get; set; } = "";

        public enum Models
        {
            /// <summary>
            /// This is the option you need to select if you want to use your own local model in TestingMode.
            /// </summary>
            TestingModel,
            /// <summary>
            /// Llama 7B Chat model, Q3_K_M quantized. For General chatting only. Speed depends on consumer hardware. [3.07GB] [Fast Model]
            /// </summary>
            Llama_7B_Chat,
            /// <summary>
            /// Llama Faster 7B Chat model, Q2_K quantized. For General chatting only. Speed depends on consumer hardware. [2.63GB] [Fastest Model]
            /// </summary>
            Llama_7B_Chat_Faster,
            /// <summary>
            /// DeepSeek 6B Coder model, Q3_K_M quantized. For coding tasks. Speed depends on consumer hardware. [3.07GB]
            /// </summary>
            DeepSeek_6B_Coder,
            /// <summary>
            /// DeepSeek 7B Chat model, Q6_K quantized. For General chatting only. Speed depends on consumer hardware. [5.28GB] [Generally Slow model]
            /// </summary>
            DeepSeek_7B_Chat,
            /// <summary>
            /// Vision 1 Text-Image-To-Text model, Q2_K quantized. For image processing tasks. Speed depends on consumer hardware. [1.5GB]
            /// </summary>
            Vision1_Q2_K,
        }

        public enum AuthState
        {
            Authenticated,
            NotAuthenticated,
            AlreadyAuthenticated,
            DownloadingModel,
            DownloadFailed
        }

        public enum DownloadStatus
        {
            NotStarted,
            InProgress,
            Completed,
            Failed
        }

        public static DownloadStatus CurrentDownloadStatus { get; private set; } = DownloadStatus.NotStarted;
        public static string CurrentDownloadProgress { get; private set; } = "0%";

        public class ModelInfo
        {
            public string FileName { get; set; }
            public string DownloadUrl { get; set; }
            public long ExpectedFileSizeBytes { get; set; }

            public string GetLocalPath()
            {
                string folder = Path.Combine(Environment.GetFolderPath(Environment.SpecialFolder.ApplicationData), "VeloraAI");
                return Path.Combine(folder, FileName);
            }
        }

        private static readonly Dictionary<Models, ModelInfo> ModelInfos = new()
        {
            [Models.Llama_7B_Chat] = new ModelInfo
            {
                FileName = "Llama_7B_Chat.gguf",
                DownloadUrl = "https://huggingface.co/ZiADKY/VeloraAI_SupportedModels/resolve/main/Llama_7B_Chat.gguf?download=true",
                ExpectedFileSizeBytes = 3298004672L
            },

            [Models.Llama_7B_Chat_Faster] = new ModelInfo
            {
                FileName = "Llama_7B_Chat_Faster.gguf",
                DownloadUrl = "https://huggingface.co/ZiADKY/VeloraAI_SupportedModels/resolve/main/Llama_7B_Chat_Faster.gguf",
                ExpectedFileSizeBytes = 2825940736L
            },
            [Models.DeepSeek_6B_Coder] = new ModelInfo
            {
                FileName = "DeepSeek_6B_Coder.gguf",
                DownloadUrl = "https://huggingface.co/ZiADKY/VeloraAI_SupportedModels/resolve/main/DeepSeek_6B_Coder.gguf?download=true",
                ExpectedFileSizeBytes = 3299880960L
            },
            [Models.DeepSeek_7B_Chat] = new ModelInfo
            {
                FileName = "DeepSeek_7B_Chat.gguf",
                DownloadUrl = "https://huggingface.co/ZiADKY/VeloraAI_SupportedModels/resolve/main/DeepSeek_7B_Chat.gguf?download=true",
                ExpectedFileSizeBytes = 5673852704L
            },
            [Models.Vision1_Q2_K] = new ModelInfo
            {
                FileName = "Vision1_Q2_K.gguf",
                DownloadUrl = "https://huggingface.co/ZiADKY/VeloraAI_SupportedModels/resolve/main/Vision1_Q2_K.gguf?download=true",
                ExpectedFileSizeBytes = 3179137120L
            },
        };

        public static async Task<AuthState> AuthenticateAsync(Models modelName, string systemPrompt = "You are VELORA Chatbot.")
        {
            if (Authenticated)
                return AuthState.AlreadyAuthenticated;

            string modelPath;

            if (TestingMode)
            {
                if (string.IsNullOrWhiteSpace(TestingModelPath) || !File.Exists(TestingModelPath))
                {
                    Console.WriteLine("TestingModelPath is not set or file does not exist.");
                    Debug.WriteLine("TestingModelPath is not set or file does not exist.");
                    return AuthState.NotAuthenticated;
                }
                modelPath = TestingModelPath;
            }
            else
            {
                if (!ModelInfos.TryGetValue(modelName, out ModelInfo modelInfo))
                {
                    Console.WriteLine($"Unknown model requested: {modelName}");
                    Debug.WriteLine($"Unknown model requested: {modelName}");
                    return AuthState.NotAuthenticated;
                }

                string folder = Path.GetDirectoryName(modelInfo.GetLocalPath())!;
                Directory.CreateDirectory(folder);

                modelPath = modelInfo.GetLocalPath();

                if (File.Exists(modelPath))
                {
                    var fileInfo = new FileInfo(modelPath);
                    if (fileInfo.Length < modelInfo.ExpectedFileSizeBytes)
                    {
                        try { fileInfo.Delete(); } catch { Debug.WriteLine($"Failed to delete incomplete model file: {fileInfo.Name}"); return AuthState.DownloadFailed; }
                    }
                }

                if (!File.Exists(modelPath))
                {
                    CurrentDownloadStatus = DownloadStatus.InProgress;
                    var downloadSuccess = await DownloadModelAsync(modelInfo.DownloadUrl, modelPath);
                    if (!downloadSuccess)
                    {
                        CurrentDownloadStatus = DownloadStatus.Failed;
                        return AuthState.DownloadFailed;
                    }
                    CurrentDownloadStatus = DownloadStatus.Completed;
                }
            }

            _modelPath = modelPath;

            try
            {
                var modelParams = new ModelParams(modelPath);
                var weights = LLamaWeights.LoadFromFile(modelParams);
                _context = weights.CreateContext(modelParams);
                _executor = new InteractiveExecutor(_context);
                _session = CreateNewSession(systemPrompt);

                var hideWords = new LLamaTransforms.KeywordTextOutputStreamTransform(new[] { "User:", "Bot:" });
                _session.WithOutputTransform(hideWords);

                Authenticated = true;
                _currentModel = modelName;
                return AuthState.Authenticated;
            }
            catch (Exception ex)
            {
                Console.WriteLine("Authentication failed: " + ex.Message);
                Debug.WriteLine("Authentication failed: " + ex.Message);
                if (File.Exists(_modelPath)) { try { File.Delete(_modelPath); } catch { } }
                return AuthState.NotAuthenticated;
            }
        }

        private static readonly HttpClient client = new();

        private static async Task<bool> DownloadModelAsync(string url, string outputPath)
        {
            try
            {
                using var response = await client.GetAsync(url, HttpCompletionOption.ResponseHeadersRead);
                response.EnsureSuccessStatusCode();

                var contentLength = response.Content.Headers.ContentLength;

                using var stream = await response.Content.ReadAsStreamAsync();
                using var fileStream = new FileStream(outputPath, FileMode.Create, FileAccess.Write, FileShare.None, 8192, true);

                var buffer = new byte[8192];
                long totalRead = 0;
                int read;
                int lastPrintedPercent = -1;

                while ((read = await stream.ReadAsync(buffer, 0, buffer.Length)) > 0)
                {
                    await fileStream.WriteAsync(buffer, 0, read);
                    totalRead += read;

                    if (contentLength.HasValue)
                    {
                        int progressPercent = (int)((totalRead * 100) / contentLength.Value);
                        if (progressPercent != lastPrintedPercent)
                        {
                            lastPrintedPercent = progressPercent;
                            CurrentDownloadProgress = progressPercent + "%";
                            Console.WriteLine($"Download progress: {CurrentDownloadProgress}");
                            Debug.WriteLine($"Download progress: {CurrentDownloadProgress}");
                        }
                    }
                }

                CurrentDownloadProgress = "Finished";
                return true;
            }
            catch (Exception ex)
            {
                Console.WriteLine("Error downloading model: " + ex.Message);
                Debug.WriteLine("Error downloading model: " + ex.Message);
                CurrentDownloadProgress = "Error";
                return false;
            }
        }

        private static ChatSession CreateNewSession(string systemPrompt)
        {
            var session = new ChatSession(_executor);
            session.History.AddMessage(AuthorRole.System, systemPrompt);
            return session;
        }

        public static void ResetHistory(string newSystemPrompt = "You are VELORA Chatbot.")
        {
            _session = CreateNewSession(newSystemPrompt);
        }

        /// <summary>
        /// Ask a question to the chosen authenticated model or use the TestingModel if TestingMode is enabled.
        /// </summary>
        public static async Task AskAsync(string userInput,
            float temperature = 0.8f,
            float TopP = 0.95f,
            int TopK = 40,
            float RepeatPenalty = 1.1f,
            string[]? AntiPrompts = null,
            bool NoBSMode = false)
        {
            if (_session == null)
            {
                Console.WriteLine("❌ Error: Model is not authenticated.");
                Debug.WriteLine("❌ Error: Model is not authenticated.");
                return;
            }

            AntiPrompts ??= new[] { "User:" };

            ResponseStarted?.Invoke(null, EventArgs.Empty);

            var infParams = new InferenceParams
            {
                SamplingPipeline = new DefaultSamplingPipeline
                {
                    Temperature = temperature,
                    TopP = TopP,
                    TopK = TopK,
                    RepeatPenalty = RepeatPenalty,
                },
                AntiPrompts = AntiPrompts
            };

            var userMessage = new ChatHistory.Message(AuthorRole.User, userInput);

            await foreach (var text in _session.ChatAsync(userMessage, infParams))
            {
                TextGenerated?.Invoke(null, text);
            }

            ResponseEnded?.Invoke(null, EventArgs.Empty);
        }

        /// <summary>
        /// Ask the Vision1 model a question using a base64 image input.
        /// This method only works if the authenticated model is Vision1_Q2_K.
        /// </summary>
        public static async Task AskWithImageAsync(string imagePath, string promptText,
            float temperature = 0.8f,
            float TopP = 0.95f,
            int TopK = 40,
            float RepeatPenalty = 1.1f,
            string[]? AntiPrompts = null)
        {
            if (_session == null)
            {
                Console.WriteLine("❌ Error: Model is not authenticated.");
                Debug.WriteLine("❌ Error: Model is not authenticated.");
                return;
            }

            if (!File.Exists(imagePath))
            {
                Console.WriteLine("❌ Error: Provided image path does not exist.");
                Debug.WriteLine("❌ Error: Provided image path does not exist.");
                return;
            }

            if (_currentModel != Models.Vision1_Q2_K)
            {
                Console.WriteLine("❌ Error: Vision1_Q2_K model is not currently authenticated.");
                Debug.WriteLine("❌ Error: Vision1_Q2_K model is not currently authenticated.");
                return;
            }

            // Convert image to base64 string
            string base64Image = Convert.ToBase64String(File.ReadAllBytes(imagePath));
            string imagePrefix = "<image:" + base64Image + ">\n";
            string finalPrompt = imagePrefix + promptText;

            AntiPrompts ??= new[] { "User:" };

            ResponseStarted?.Invoke(null, EventArgs.Empty);

            var infParams = new InferenceParams
            {
                SamplingPipeline = new DefaultSamplingPipeline
                {
                    Temperature = temperature,
                    TopP = TopP,
                    TopK = TopK,
                    RepeatPenalty = RepeatPenalty,
                },
                AntiPrompts = AntiPrompts
            };

            var userMessage = new ChatHistory.Message(AuthorRole.User, finalPrompt);

            await foreach (var text in _session.ChatAsync(userMessage, infParams))
            {
                TextGenerated?.Invoke(null, text);
            }

            ResponseEnded?.Invoke(null, EventArgs.Empty);
        }

        public static AuthState IsAuthenticated()
        {
            return Authenticated ? AuthState.Authenticated : AuthState.NotAuthenticated;
        }
    }
}